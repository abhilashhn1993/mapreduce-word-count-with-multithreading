{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MapReduce.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNVSOExm9TovaxkdHTWIpPm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abhilashhn1993/word_count_with_mapreduce/blob/master/MapReduce.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qktTLFUeTNrm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "import threading\n",
        "import queue\n",
        "import pandas as pd"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fBQtzfYDXXD_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nCNSP9dcYucx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Reading the Input file\n",
        "file = open(r\"/content/Input.txt\")"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b1B6WlNsT3Bd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Implementing methods to clean function\n",
        "def cleanText(file):\n",
        "   words = []\n",
        "   reg = re.compile(\"[a-z]+\")\n",
        "   for line in file:\n",
        "     #token = line.split()\n",
        "     #words.append(token)\n",
        "     #for line in words:\n",
        "     if len(line) > 0:\n",
        "         words.append(reg.findall(line.lower()))\n",
        "   words = [i for i in words if i!=[]]\n",
        "   return words"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RRtmDlI3iH4t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def unlist(input):\n",
        "  list = []\n",
        "  for i in input:\n",
        "    for word in i:\n",
        "      list.append(word)\n",
        "  return list"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GT_F1-YIVV-Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Data split function to divide the input data file into two halves\n",
        "#First part includes the first 5000 lines of the text file and part 2 includes the rest\n",
        "def split(token):\n",
        "  words = []\n",
        "  for word in token:\n",
        "    words.append(word)\n",
        "  file1 = unlist(words[0:5000])\n",
        "  file2 = unlist(words[5001:])\n",
        "  files = [file1, file2]\n",
        "  return files"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xweqXiVgftqF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Mapper function\n",
        "def mapper1(list, mapper_q1):\n",
        "  mapping = []\n",
        "  for word in list:\n",
        "    mapping.append(\"%s%s%d\"%(word,',',1))\n",
        "  mapper_q1.put(mapping)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jwYRpNJc5_M9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def mapper2(list, mapper_q2):\n",
        "  mapping = []\n",
        "  for word in list:\n",
        "    mapping.append(\"%s%s%d\"%(word,',', 1))\n",
        "  mapper_q2.put(mapping)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8T3LG2-oiRE-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Function to sort the mapped words\n",
        "def sort_func(map1, map2):\n",
        "  map = map1 + map2\n",
        "  map.sort()\n",
        "  return map"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ipznxIDx7Tp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Partition function to divide the tokens into two halves\n",
        "# a-m words and n-z words\n",
        "def partition(list):\n",
        "  file1 = []\n",
        "  file2 = []\n",
        "  for item in list:\n",
        "    val = re.search(\"^[a-m]\", item)\n",
        "    if val!= None:\n",
        "      file1.append(item)\n",
        "    else:\n",
        "      file2.append(item)\n",
        "  file = [file1,file2]\n",
        "  return file"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JVSqyx9ZheWR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Reducer function\n",
        "def reducer1(sortedList, reducer_q1):\n",
        "  word_count = {}\n",
        "  item = None\n",
        "  current_count = 0\n",
        "  for words in sortedList:\n",
        "    word,count = words.split(',',1)\n",
        "    count = int(count)\n",
        "    \n",
        "    if item == word:\n",
        "      current_count += count\n",
        "    else:\n",
        "      item = word\n",
        "      current_count = count\n",
        "    word_count[item] = current_count\n",
        "  if item == word:\n",
        "    word_count[item] = current_count\n",
        "  return reducer_q1.put(word_count)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pTb8isQToTnd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def reducer2(sortedList, reducer_q2):\n",
        "  word_count = {}\n",
        "  item = None\n",
        "  current_count = 0\n",
        "  for words in sortedList:\n",
        "    word,count = words.split(',',1)\n",
        "    count = int(count)\n",
        "    \n",
        "    if item == word:\n",
        "      current_count += count\n",
        "    else:\n",
        "      item = word\n",
        "      current_count = count\n",
        "    word_count[item] = current_count\n",
        "  if item == word:\n",
        "    word_count[item] = current_count\n",
        "  return reducer_q2.put(word_count)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CNhp8b4bDuA9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Main method calling the MapReduce function\n",
        "def MapReduce(path):\n",
        "  mapper_q1 = queue.Queue()\n",
        "  mapper_q2 = queue.Queue()\n",
        "  reducer_q1 = queue.Queue()\n",
        "  reducer_q2 = queue.Queue()\n",
        "\n",
        "  wordTokens = cleanText(file)\n",
        "  filesList = split(wordTokens)\n",
        "\n",
        "  mapping1 = threading.Thread(target=mapper1, args=(filesList[0], mapper_q1))\n",
        "  mapping2 = threading.Thread(target=mapper2, args=(filesList[1], mapper_q2))\n",
        "  mapping1.start()\n",
        "  mapping2.start()\n",
        "  mapping1.join()\n",
        "  mapping2.join()\n",
        "\n",
        "  mapping1 =[]\n",
        "  for element in mapper_q1.get():\n",
        "    mapping1.append(element)\n",
        "\n",
        "  mapping2 =[]\n",
        "  for element in mapper_q2.get():\n",
        "    mapping2.append(element)\n",
        "\n",
        "  sortedMap = sort_func(mapping1, mapping2)\n",
        "  sortedFile = partition(sortedMap)\n",
        "\n",
        "  reducer1 = threading.Thread(target=reducer1, args = (sortedFile[0], reducer_q1))\n",
        "  reducer2 = threading.Thread(target=reducer2, args = (sortedFile[1], reducer_q2))\n",
        "\n",
        "  reducer1.start()\n",
        "  reducer2.start()\n",
        "  reducer1.join()\n",
        "  reducer2.join()\n",
        "\n",
        "  reducer1 = reducer_q1.get()\n",
        "  reducer2 = reducer_q2.get()\n",
        "\n",
        "  reducer1.update(reducer2)\n",
        "  output_file = pd.DataFrame(reducer1.items(), columns = ['Word', 'Count'])\n",
        "  output_file.to_csv('Output.csv')"
      ],
      "execution_count": 13,
      "outputs": []
    }
  ]
}